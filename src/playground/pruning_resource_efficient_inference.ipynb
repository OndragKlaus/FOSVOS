{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for paper \"Pruning Convolutional Neural Networks for Resource Efficient Inference\"\n",
    "# code adopted from https://github.com/eeric/channel_prune\n",
    "# which itself is adopted from https://github.com/jacobgil/pytorch-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from typing import Optional\n",
    "import operator\n",
    "import heapq\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.path[0] != '..':\n",
    "    sys.path.insert(0, '..')\n",
    "\n",
    "path_ros = '/opt/ros/kinetic/lib/python2.7/dist-packages'\n",
    "if path_ros in sys.path:\n",
    "    del sys.path[sys.path.index(path_ros)]\n",
    "\n",
    "from networks.osvos_resnet import OSVOS_RESNET\n",
    "from util import io_helper, experiment_helper\n",
    "from layers.osvos_layers import class_balanced_cross_entropy_loss, center_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net() -> nn.Module:\n",
    "    net = OSVOS_RESNET(pretrained=False)\n",
    "    path_model = Path('../models/resnet18_11_11_blackswan_epoch-9999.pth')\n",
    "    parameters = torch.load(str(path_model), map_location=lambda storage, loc: storage)\n",
    "    net.load_state_dict(parameters)\n",
    "    net = net.cuda()\n",
    "    return net\n",
    "\n",
    "\n",
    "net = get_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_num_filters(net: nn.Module) -> int:\n",
    "    n_filters = 0\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            n_filters += m.out_channels\n",
    "        if n_filters > 0:\n",
    "            return n_filters\n",
    "    return n_filters\n",
    "\n",
    "\n",
    "n_filters = total_num_filters(net)\n",
    "# n_filters_to_prune_per_iter = 512\n",
    "n_filters_to_prune_per_iter = 8\n",
    "n_iterations = int(n_filters / n_filters_to_prune_per_iter * 2 / 3)\n",
    "\n",
    "print('Filters in model:', n_filters)\n",
    "print('Prune n filters per iteration:', n_filters_to_prune_per_iter)\n",
    "print('Number of iterations:', n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlockDummy(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, conv1, bn1, relu, conv2, bn2, downsample, stride):\n",
    "        super(BasicBlockDummy, self).__init__()\n",
    "        self.conv1 = conv1\n",
    "        self.bn1 = bn1\n",
    "        self.relu = relu\n",
    "        self.conv2 = conv2\n",
    "        self.bn2 = bn2\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterPruner:\n",
    "    def __init__(self, net: nn.Module):\n",
    "        self.net = net\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.filter_ranks = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = []\n",
    "        self.gradients = []\n",
    "        self.grad_index = 0\n",
    "        self.activation_to_layer = {}\n",
    "        activation_index = 0\n",
    "        kk = 0\n",
    "\n",
    "        crop_h, crop_w = int(x.size()[-2]), int(x.size()[-1])\n",
    "\n",
    "        for l in self.net.layer_base:\n",
    "            x = l(x)\n",
    "            if isinstance(l, torch.nn.modules.conv.Conv2d):\n",
    "                x.register_hook(self.compute_rank)\n",
    "                self.activations.append(x)\n",
    "                self.activation_to_layer[activation_index] = kk\n",
    "                activation_index += 1\n",
    "                kk += 1\n",
    "        # x = self.net.layer_base(x)\n",
    "\n",
    "        side = []\n",
    "        side_out = []\n",
    "        for (layer_stage, layer_side_prep, layer_upscale_side_prep,\n",
    "             layer_score_dsn, layer_upscale_score_dsn) in zip(self.net.layer_stages, self.net.side_prep,\n",
    "                                                              self.net.upscale_side_prep,\n",
    "                                                              self.net.score_dsn, self.net.upscale_score_dsn):\n",
    "            x = layer_stage(x)\n",
    "            temp_side_prep = layer_side_prep(x)\n",
    "\n",
    "            temp_upscale = layer_upscale_side_prep(temp_side_prep)\n",
    "            temp_cropped = center_crop(temp_upscale, crop_h, crop_w)\n",
    "            side.append(temp_cropped)\n",
    "\n",
    "            temp_score_dsn = layer_score_dsn(temp_side_prep)\n",
    "            temp_upscale_ = layer_upscale_score_dsn(temp_score_dsn)\n",
    "            temp_cropped_ = center_crop(temp_upscale_, crop_h, crop_w)\n",
    "            side_out.append(temp_cropped_)\n",
    "\n",
    "        out = torch.cat(side[:], dim=1)\n",
    "        out = self.net.layer_fuse(out)\n",
    "        side_out.append(out)\n",
    "        return side_out\n",
    "\n",
    "    def compute_rank(self, grad):\n",
    "        activation_index = len(self.activations) - self.grad_index - 1\n",
    "        activation = self.activations[activation_index]\n",
    "        values = torch.sum((activation * grad), dim=0, keepdim=True).sum(dim=2, keepdim=True).sum(dim=3, keepdim=True)[\n",
    "                 0, :, 0, 0].data\n",
    "\n",
    "        # Normalize the rank by the filter dimensions\n",
    "        values = values / (activation.size(0) * activation.size(2) * activation.size(3))\n",
    "\n",
    "        if activation_index not in self.filter_ranks:\n",
    "            self.filter_ranks[activation_index] = torch.FloatTensor(activation.size(1)).zero_().cuda()\n",
    "            # self.filter_ranks[activation_index] = torch.FloatTensor(activation.size(1)).zero_()\n",
    "\n",
    "        self.filter_ranks[activation_index] += values\n",
    "        self.grad_index += 1\n",
    "\n",
    "    def normalize_ranks_per_layer(self):\n",
    "        for i in self.filter_ranks:\n",
    "            v = torch.abs(self.filter_ranks[i])\n",
    "            v = v / np.sqrt(torch.sum(v * v))\n",
    "            self.filter_ranks[i] = v.cpu()\n",
    "\n",
    "    def lowest_ranking_filters(self, n_filters_to_prune_per_iter):\n",
    "        data = []\n",
    "        for i in sorted(self.filter_ranks.keys()):\n",
    "            for j in range(self.filter_ranks[i].size(0)):\n",
    "                data.append((self.activation_to_layer[i], j, self.filter_ranks[i][j]))\n",
    "\n",
    "        return heapq.nsmallest(n_filters_to_prune_per_iter, data, operator.itemgetter(2))\n",
    "\n",
    "    def get_prunning_plan(self, n_filters_to_prune_per_iter):\n",
    "        filters_to_prune = self.lowest_ranking_filters(n_filters_to_prune_per_iter)\n",
    "\n",
    "        # After each of the k filters are prunned,\n",
    "        # the filter index of the next filters change since the model is smaller.\n",
    "        filters_to_prune_per_layer = {}\n",
    "        for (l, f, _) in filters_to_prune:\n",
    "            if l not in filters_to_prune_per_layer:\n",
    "                filters_to_prune_per_layer[l] = []\n",
    "            filters_to_prune_per_layer[l].append(f)\n",
    "\n",
    "        for l in filters_to_prune_per_layer:\n",
    "            filters_to_prune_per_layer[l] = sorted(filters_to_prune_per_layer[l])\n",
    "            for i in range(len(filters_to_prune_per_layer[l])):\n",
    "                filters_to_prune_per_layer[l][i] = filters_to_prune_per_layer[l][i] - i\n",
    "\n",
    "        filters_to_prune = []\n",
    "        for l in filters_to_prune_per_layer:\n",
    "            for i in filters_to_prune_per_layer[l]:\n",
    "                filters_to_prune.append((l, i))\n",
    "\n",
    "        return filters_to_prune\n",
    "\n",
    "\n",
    "pruner = FilterPruner(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = io_helper.get_data_loader_test(Path('/home/klaus/dev/datasets/DAVIS'), batch_size=1, seq_name='blackswan')\n",
    "# data_loader = io_helper.get_data_loader_test(Path('/usr/stud/ondrag/DAVIS'), batch_size=1, seq_name='blackswan')\n",
    "\n",
    "\n",
    "def train(pruner: FilterPruner, data_loader: data.DataLoader, n_epochs: Optional[int] = 1) -> None:\n",
    "    for epoch in range(n_epochs):\n",
    "        for minibatch in data_loader:\n",
    "            pruner.net.zero_grad()\n",
    "            inputs, gts = minibatch['image'], minibatch['gt']\n",
    "            inputs, gts = Variable(inputs), Variable(gts)\n",
    "            inputs, gts = inputs.cuda(), gts.cuda()\n",
    "\n",
    "            outputs = pruner.forward(inputs)\n",
    "            loss = class_balanced_cross_entropy_loss(outputs[-1], gts, size_average=False)\n",
    "            loss.backward()\n",
    "            return\n",
    "\n",
    "\n",
    "def fine_tune(net: nn.Module(), data_loader: data.DataLoader, n_epochs: Optional[int] = 1) -> None:\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=0.0002)\n",
    "    avg_grad_every_n = 5\n",
    "    counter_gradient = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for minibatch in data_loader:\n",
    "            net.zero_grad()\n",
    "            inputs, gts = minibatch['image'], minibatch['gt']\n",
    "            inputs, gts = Variable(inputs), Variable(gts)\n",
    "            inputs, gts = inputs.cuda(), gts.cuda()\n",
    "\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = class_balanced_cross_entropy_loss(outputs[-1], gts, size_average=False)\n",
    "            loss /= avg_grad_every_n\n",
    "            loss.backward()\n",
    "            counter_gradient += 1\n",
    "\n",
    "            if counter_gradient % avg_grad_every_n == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_resnet18_conv_layer(net, layer_index, filter_index):\n",
    "    # fix missing bias\n",
    "    next_conv = None\n",
    "    next_new_conv = None\n",
    "    downin_conv = None\n",
    "    downout_conv = None\n",
    "    next_downin_conv = None\n",
    "    new_down_conv = None\n",
    "\n",
    "    if layer_index == 0:\n",
    "        conv = net.layer_base[0]\n",
    "        next_conv = net.layer_stages[0][0].conv1\n",
    "\n",
    "    new_conv = torch.nn.Conv2d(in_channels=conv.in_channels,\n",
    "                               out_channels=conv.out_channels - 1,\n",
    "                               kernel_size=conv.kernel_size,\n",
    "                               stride=conv.stride,\n",
    "                               padding=conv.padding,\n",
    "                               dilation=conv.dilation,\n",
    "                               groups=conv.groups,\n",
    "                               bias=conv.bias)\n",
    "\n",
    "    old_weights = conv.weight.data.cpu().numpy()\n",
    "    new_weights = new_conv.weight.data.cpu().numpy()\n",
    "\n",
    "    new_weights[:filter_index, :, :, :] = old_weights[:filter_index, :, :, :]\n",
    "    new_weights[filter_index:, :, :, :] = old_weights[filter_index + 1:, :, :, :]\n",
    "    new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
    "    # new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "\n",
    "    if next_conv is not None:\n",
    "        next_new_conv = torch.nn.Conv2d(in_channels=next_conv.in_channels - 1,\n",
    "                                        out_channels=next_conv.out_channels,\n",
    "                                        kernel_size=next_conv.kernel_size,\n",
    "                                        stride=next_conv.stride,\n",
    "                                        padding=next_conv.padding,\n",
    "                                        dilation=next_conv.dilation,\n",
    "                                        groups=next_conv.groups,\n",
    "                                        bias=next_conv.bias)\n",
    "\n",
    "        old_weights = next_conv.weight.data.cpu().numpy()\n",
    "        new_weights = next_new_conv.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:, :filter_index, :, :] = old_weights[:, :filter_index, :, :]\n",
    "        new_weights[:, filter_index:, :, :] = old_weights[:, filter_index + 1:, :, :]\n",
    "        next_new_conv.weight.data = torch.from_numpy(new_weights).cuda()\n",
    "        # next_new_conv.weight.data = torch.from_numpy(new_weights)\n",
    "\n",
    "        if not next_conv is None:\n",
    "            if layer_index == 0:\n",
    "                net.layer_base = nn.Sequential(new_conv, *list(net.layer_base.children())[1:])\n",
    "\n",
    "                downsample = nn.Sequential(nn.Conv2d(next_new_conv.in_channels, next_new_conv.out_channels,\n",
    "                                                     kernel_size=1, stride=1, bias=False),\n",
    "                                           nn.BatchNorm2d(next_new_conv.out_channels))\n",
    "                bb = net.layer_stages[0][0]\n",
    "                net.layer_stages[0] = nn.Sequential(BasicBlockDummy(next_new_conv, bb.bn1, bb.relu,\n",
    "                                                                    bb.conv2, bb.bn2, downsample, bb.stride),\n",
    "                                                    net.layer_stages[0][1])\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def batchnorm_modify(net, layer_index, filter_index):\n",
    "    if layer_index == 0:\n",
    "        batchnorm_old = net.layer_base[1]\n",
    "        new_batchnorm = nn.BatchNorm2d(num_features=batchnorm_old.num_features - 1,\n",
    "                                       eps=batchnorm_old.eps,\n",
    "                                       momentum=batchnorm_old.momentum,\n",
    "                                       affine=batchnorm_old.affine)\n",
    "        # net.layer_base[1].track_running_stats no attribute...\n",
    "\n",
    "        old_weights = batchnorm_old.weight.data.cpu().numpy()\n",
    "        new_weights = new_batchnorm.weight.data.cpu().numpy()\n",
    "\n",
    "        new_weights[:filter_index] = old_weights[:filter_index]\n",
    "        new_weights[filter_index:] = old_weights[filter_index + 1:]\n",
    "        new_batchnorm.weight.data = torch.from_numpy(new_weights).cuda()\n",
    "        # new_batchnorm.weight.data = torch.from_numpy(new_weights)\n",
    "        # 'layer_base.1.weight', 'layer_base.1.bias', 'layer_base.1.running_mean', 'layer_base.1.running_var'\n",
    "        children = list(net.layer_base.children())\n",
    "        net.layer_base = nn.Sequential(children[0], new_batchnorm, *children[2:])\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates_to_prune(pruner: FilterPruner, n_filters_to_prune: int, net: nn.Module,\n",
    "                            data_loader: data.DataLoader):\n",
    "    pruner.reset()\n",
    "    train(pruner, data_loader)\n",
    "    pruner.normalize_ranks_per_layer()\n",
    "    return pruner.get_prunning_plan(n_filters_to_prune)\n",
    "\n",
    "for _ in tqdm(range(n_iterations)):\n",
    "    # print('Ranking filters')\n",
    "    prune_targets = get_candidates_to_prune(pruner, n_filters_to_prune_per_iter, net, data_loader)\n",
    "    layers_prunned = {}\n",
    "    for layer_index, filter_index in prune_targets:\n",
    "        if layer_index not in layers_prunned:\n",
    "            layers_prunned[layer_index] = 0\n",
    "        layers_prunned[layer_index] = layers_prunned[layer_index] + 1\n",
    "\n",
    "    # print(\"Layers that will be prunned\", layers_prunned)\n",
    "    # print(\"Prunning filters.. \")\n",
    "    net = net.cpu()\n",
    "    for layer_index, filter_index in prune_targets:\n",
    "        net = prune_resnet18_conv_layer(net, layer_index, filter_index)\n",
    "        net = batchnorm_modify(net, layer_index, filter_index)\n",
    "\n",
    "    net = net.cuda()\n",
    "    # print(\"Plan to prune...\", net)\n",
    "\n",
    "    # message = str(100 * total_num_filters(net) / n_filters) + \"%\"\n",
    "    # print(\"Filters prunned\", str(message))\n",
    "    # print(\"Fine tuning to recover from prunning iteration.\")\n",
    "    fine_tune(net, data_loader, n_epochs=10)\n",
    "\n",
    "path_export = Path('../models/resnet18_11_11_blackswan_epoch-9999_min.pth')\n",
    "torch.save(net.state_dict(), str(path_export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyProvider:\n",
    "    def __init__(self, net):\n",
    "        self.network = net\n",
    "\n",
    "\n",
    "net_provider = DummyProvider(net)\n",
    "# first time to measure the speed\n",
    "experiment_helper.test(net_provider, data_loader, Path('../results/resnet18/11/11_min'), is_visualizing_results=False,\n",
    "                       eval_speeds=True, seq_name='blackswan')\n",
    "\n",
    "# second time for image output\n",
    "experiment_helper.test(net_provider, data_loader, Path('../results/resnet18/11/11_min'), is_visualizing_results=False,\n",
    "                       eval_speeds=False, seq_name='blackswan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
